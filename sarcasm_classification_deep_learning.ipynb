{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.core import Dropout, Dense, Activation, Flatten, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from collections import defaultdict\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193517, 50)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"embeddings/glove/glove.twitter.27B.50d.txt\", word2vec_output_file=\"embeddings/glove/gensim_glove.twitter.27B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format(\"embeddings/glove/gensim_glove.twitter.27B.50d.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train.jsonl') as f:\n",
    "    train_data = pd.read_json(f, lines=True)\n",
    "with open('data/test.jsonl') as f:\n",
    "    test_data = pd.read_json(f, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(train_data['label'])\n",
    "y_train = np_utils.to_categorical(y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['context'] = train_data['context'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH_RESPONSE = 100\n",
    "MAX_SEQUENCE_LENGTH_CONTEXT = 200\n",
    "MAX_SEQUENCE_LENGTH = MAX_SEQUENCE_LENGTH_RESPONSE + MAX_SEQUENCE_LENGTH_CONTEXT\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower = False)\n",
    "tokenizer.fit_on_texts(train_data['response'])\n",
    "tokenizer.fit_on_texts(train_data['context'])\n",
    "tokenizer.fit_on_texts(test_data['response'])\n",
    "tokenizer.fit_on_texts(test_data['context'])\n",
    "X_train_resp = pad_sequences(tokenizer.texts_to_sequences(train_data['response']), maxlen=MAX_SEQUENCE_LENGTH_RESPONSE)\n",
    "X_train_cont = pad_sequences(tokenizer.texts_to_sequences(train_data['context']), maxlen=MAX_SEQUENCE_LENGTH_CONTEXT)\n",
    "#X_train = np.hstack((X_train_resp, X_train_cont))\n",
    "X_test_resp = pad_sequences(tokenizer.texts_to_sequences(test_data['response']), maxlen=MAX_SEQUENCE_LENGTH_RESPONSE)\n",
    "X_test_cont = pad_sequences(tokenizer.texts_to_sequences(test_data['context']), maxlen=MAX_SEQUENCE_LENGTH_CONTEXT)\n",
    "#X_test = np.hstack((X_test_resp, X_test_cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46848 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = glove_model.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in glove_model.vocab:\n",
    "        embedding_matrix[i] = glove_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of parameter: 4940002\n",
      "Model: \"functional_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_59 (InputLayer)           [(8, 100)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_60 (InputLayer)           [(8, 200)]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_82 (Embedding)        (8, 100, 50)         2342450     input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_83 (Embedding)        (8, 200, 50)         2342450     input_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (8, 300, 50)         0           embedding_82[0][0]               \n",
      "                                                                 embedding_83[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (8, 298, 50)         7550        concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (8, 296, 50)         7550        conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (8, 296, 50)         0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_37 (LSTM)                  (8, 296, 128)        91648       dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_38 (LSTM)                  (8, 128)             131584      lstm_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (8, 128)             16512       lstm_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (8, 2)               258         dense_34[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,940,002\n",
      "Trainable params: 255,102\n",
      "Non-trainable params: 4,684,900\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "input_response = keras.Input(batch_shape=(batch_size, MAX_SEQUENCE_LENGTH_RESPONSE), dtype=\"int32\")\n",
    "embed_response = layers.Embedding(len(word_index) + 1,\n",
    "                          EMBEDDING_DIM,\n",
    "                          weights=[embedding_matrix],\n",
    "                          input_length=MAX_SEQUENCE_LENGTH_RESPONSE,\n",
    "                          trainable=False)(input_response)\n",
    "input_context = keras.Input(batch_shape=(batch_size, MAX_SEQUENCE_LENGTH_CONTEXT), dtype=\"int32\")\n",
    "embed_context = layers.Embedding(len(word_index) + 1,\n",
    "                          EMBEDDING_DIM,\n",
    "                          weights=[embedding_matrix],\n",
    "                          input_length=MAX_SEQUENCE_LENGTH_CONTEXT,\n",
    "                          trainable=False)(input_context)\n",
    "x = layers.Concatenate(axis = 1)([embed_response ,embed_context])\n",
    "x = layers.Conv1D(EMBEDDING_DIM, 3, kernel_initializer='he_normal', padding='valid',\n",
    "                        activation='sigmoid')(x)\n",
    "x = layers.Conv1D(EMBEDDING_DIM, 3, kernel_initializer='he_normal', padding='valid',\n",
    "                        activation='sigmoid')(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "x = layers.LSTM(128, kernel_initializer='he_normal', activation='sigmoid', dropout=0.5,\n",
    "               return_sequences=True)(x)\n",
    "x = layers.LSTM(128, kernel_initializer='he_normal', activation='sigmoid', dropout=0.5)(x)\n",
    "x = layers.Dense(128, kernel_initializer='he_normal', activation='sigmoid')(x)\n",
    "predictions = layers.Dense(2, activation='softmax')(x)\n",
    "model = keras.Model([input_response, input_context], predictions)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print('No of parameter:', model.count_params())\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "500/500 [==============================] - 64s 127ms/step - loss: 0.6698 - accuracy: 0.6170 - val_loss: 1.1059 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 65s 130ms/step - loss: 0.6709 - accuracy: 0.6125 - val_loss: 1.2645 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 69s 139ms/step - loss: 0.6606 - accuracy: 0.6152 - val_loss: 0.9823 - val_accuracy: 0.2760\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 70s 140ms/step - loss: 0.6059 - accuracy: 0.6867 - val_loss: 1.1443 - val_accuracy: 0.2050\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 70s 140ms/step - loss: 0.5751 - accuracy: 0.7147 - val_loss: 1.3995 - val_accuracy: 0.1560\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 69s 137ms/step - loss: 0.5662 - accuracy: 0.7262 - val_loss: 1.3640 - val_accuracy: 0.2120\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 67s 134ms/step - loss: 0.5535 - accuracy: 0.7275 - val_loss: 1.0339 - val_accuracy: 0.3160\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 66s 131ms/step - loss: 0.5357 - accuracy: 0.7433 - val_loss: 0.9717 - val_accuracy: 0.3990\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 66s 132ms/step - loss: 0.5343 - accuracy: 0.7515 - val_loss: 0.7807 - val_accuracy: 0.5400\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 66s 132ms/step - loss: 0.5275 - accuracy: 0.7502 - val_loss: 1.0827 - val_accuracy: 0.3960\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 65s 130ms/step - loss: 0.5163 - accuracy: 0.7628 - val_loss: 1.3417 - val_accuracy: 0.2450\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 67s 134ms/step - loss: 0.5125 - accuracy: 0.7615 - val_loss: 1.0985 - val_accuracy: 0.3400\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 68s 136ms/step - loss: 0.5028 - accuracy: 0.7713 - val_loss: 1.0759 - val_accuracy: 0.3480\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 68s 136ms/step - loss: 0.4925 - accuracy: 0.7755 - val_loss: 0.8747 - val_accuracy: 0.5170\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 67s 135ms/step - loss: 0.4936 - accuracy: 0.7803 - val_loss: 1.1805 - val_accuracy: 0.3450\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 68s 137ms/step - loss: 0.4861 - accuracy: 0.7815 - val_loss: 0.7857 - val_accuracy: 0.4920\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 70s 140ms/step - loss: 0.4879 - accuracy: 0.7735 - val_loss: 1.0496 - val_accuracy: 0.3630\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 68s 136ms/step - loss: 0.4774 - accuracy: 0.7868 - val_loss: 1.0156 - val_accuracy: 0.4390\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 67s 134ms/step - loss: 0.4683 - accuracy: 0.7908 - val_loss: 0.7618 - val_accuracy: 0.5580\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 67s 134ms/step - loss: 0.4700 - accuracy: 0.7955 - val_loss: 1.0847 - val_accuracy: 0.3740\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 67s 134ms/step - loss: 0.4701 - accuracy: 0.7885 - val_loss: 1.0403 - val_accuracy: 0.3910\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 67s 134ms/step - loss: 0.4649 - accuracy: 0.7922 - val_loss: 0.9072 - val_accuracy: 0.4790\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 68s 135ms/step - loss: 0.4625 - accuracy: 0.7878 - val_loss: 1.1532 - val_accuracy: 0.4030\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 68s 136ms/step - loss: 0.4568 - accuracy: 0.7928 - val_loss: 0.7030 - val_accuracy: 0.6000\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 68s 135ms/step - loss: 0.4584 - accuracy: 0.7922 - val_loss: 1.0674 - val_accuracy: 0.3860\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 67s 134ms/step - loss: 0.4487 - accuracy: 0.8005 - val_loss: 0.9485 - val_accuracy: 0.4640\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 66s 133ms/step - loss: 0.4475 - accuracy: 0.8002 - val_loss: 1.2278 - val_accuracy: 0.3530\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 70s 140ms/step - loss: 0.4401 - accuracy: 0.7993 - val_loss: 1.1149 - val_accuracy: 0.4300\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 69s 138ms/step - loss: 0.4375 - accuracy: 0.8083 - val_loss: 1.1322 - val_accuracy: 0.4280\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 67s 134ms/step - loss: 0.4312 - accuracy: 0.8105 - val_loss: 1.0425 - val_accuracy: 0.4520\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 67s 135ms/step - loss: 0.4207 - accuracy: 0.8123 - val_loss: 1.1555 - val_accuracy: 0.3910\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 68s 136ms/step - loss: 0.4211 - accuracy: 0.8177 - val_loss: 1.1339 - val_accuracy: 0.4130\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 68s 135ms/step - loss: 0.4145 - accuracy: 0.8177 - val_loss: 0.9458 - val_accuracy: 0.4960\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 69s 137ms/step - loss: 0.4157 - accuracy: 0.8170 - val_loss: 1.1668 - val_accuracy: 0.4360\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 69s 138ms/step - loss: 0.4106 - accuracy: 0.8180 - val_loss: 0.8624 - val_accuracy: 0.5530\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 70s 140ms/step - loss: 0.4127 - accuracy: 0.8238 - val_loss: 1.2593 - val_accuracy: 0.3920\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 69s 138ms/step - loss: 0.3983 - accuracy: 0.8267 - val_loss: 1.2270 - val_accuracy: 0.4330\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 67s 135ms/step - loss: 0.3930 - accuracy: 0.8270 - val_loss: 1.3770 - val_accuracy: 0.3540\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 70s 140ms/step - loss: 0.3920 - accuracy: 0.8285 - val_loss: 1.1510 - val_accuracy: 0.4520\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 70s 140ms/step - loss: 0.3804 - accuracy: 0.8367 - val_loss: 1.2064 - val_accuracy: 0.4430\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 69s 138ms/step - loss: 0.3655 - accuracy: 0.8443 - val_loss: 1.3579 - val_accuracy: 0.4280\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 69s 137ms/step - loss: 0.3650 - accuracy: 0.8397 - val_loss: 1.3355 - val_accuracy: 0.3810\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 68s 137ms/step - loss: 0.3625 - accuracy: 0.8490 - val_loss: 1.3847 - val_accuracy: 0.4200\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 69s 137ms/step - loss: 0.3563 - accuracy: 0.8503 - val_loss: 1.5093 - val_accuracy: 0.3680\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 69s 138ms/step - loss: 0.3423 - accuracy: 0.8597 - val_loss: 0.9448 - val_accuracy: 0.5550\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 68s 137ms/step - loss: 0.3460 - accuracy: 0.8562 - val_loss: 1.4385 - val_accuracy: 0.3690\n",
      "Epoch 47/100\n",
      " 29/500 [>.............................] - ETA: 59s - loss: 0.3693 - accuracy: 0.8319"
     ]
    }
   ],
   "source": [
    "modelCheckpoint = ModelCheckpoint('best_sarcasm_model.hdf5', save_best_only = True)\n",
    "model.fit([X_train_resp, X_train_cont], y_train, batch_size=batch_size, epochs=100, validation_split=0.2 , shuffle=True, callbacks=[modelCheckpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
