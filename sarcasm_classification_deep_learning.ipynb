{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import pandas as pd\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.core import Dropout, Dense, Activation, Flatten, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from collections import defaultdict\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193517, 200)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"embeddings/glove/glove.twitter.27B.200d.txt\", word2vec_output_file=\"embeddings/glove/gensim_glove.twitter.27B.200d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format(\"embeddings/glove/gensim_glove.twitter.27B.200d.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train.jsonl') as f:\n",
    "    train_data = pd.read_json(f, lines=True)\n",
    "with open('data/test.jsonl') as f:\n",
    "    test_data = pd.read_json(f, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(train_data['label'])\n",
    "y_train = np_utils.to_categorical(y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['context'] = train_data['context'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train_data['response'])\n",
    "tokenizer.fit_on_texts(train_data['context'])\n",
    "tokenizer.fit_on_texts(test_data['response'])\n",
    "tokenizer.fit_on_texts(test_data['context'])\n",
    "X_train_resp = pad_sequences(tokenizer.texts_to_sequences(train_data['response']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train_cont = pad_sequences(tokenizer.texts_to_sequences(train_data['context']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_resp = pad_sequences(tokenizer.texts_to_sequences(test_data['response']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_cont = pad_sequences(tokenizer.texts_to_sequences(test_data['context']), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38565 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in glove_model.vocab:\n",
    "        embedding_matrix[i] = glove_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of parameter: 9013186\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 200)          7713200   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 98, 200)           120200    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 96, 200)           120200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 96, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 96, 256)           467968    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 9,013,186\n",
      "Trainable params: 1,299,986\n",
      "Non-trainable params: 7,713,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/AniSkywalker/SarcasmDetection/blob/master/src/sarcasm_detection_model_CNN_LSTM_DNN_word2vec.py\n",
    "hidden_units=256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "\n",
    "# model.add(Reshape((maxlen, emb_weights.shape[1], 1)))\n",
    "\n",
    "model.add(Convolution1D(emb_weights.shape[1], 3, kernel_initializer='he_normal', padding='valid',\n",
    "                        activation='sigmoid',\n",
    "                        input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "# model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "model.add(Convolution1D(emb_weights.shape[1], 3, kernel_initializer='he_normal', padding='valid',\n",
    "                        activation='sigmoid',\n",
    "                        input_shape=(1, MAX_SEQUENCE_LENGTH - 2)))\n",
    "# model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(hidden_units, kernel_initializer='he_normal', activation='sigmoid', dropout=0.5,\n",
    "               return_sequences=True))\n",
    "model.add(LSTM(hidden_units, kernel_initializer='he_normal', activation='sigmoid', dropout=0.5))\n",
    "\n",
    "model.add(Dense(hidden_units, kernel_initializer='he_normal', activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "print('No of parameter:', model.count_params())\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "500/500 [==============================] - 46s 91ms/step - loss: 0.6774 - acc: 0.6140 - val_loss: 1.0001 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 48s 97ms/step - loss: 0.6638 - acc: 0.6242 - val_loss: 1.2088 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 55s 110ms/step - loss: 0.6602 - acc: 0.6240 - val_loss: 1.1739 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 51s 103ms/step - loss: 0.6051 - acc: 0.6775 - val_loss: 1.0085 - val_acc: 0.4040\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 51s 101ms/step - loss: 0.5414 - acc: 0.7352 - val_loss: 1.0116 - val_acc: 0.4360\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 50s 100ms/step - loss: 0.5167 - acc: 0.7477 - val_loss: 0.7914 - val_acc: 0.5500\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 49s 97ms/step - loss: 0.5206 - acc: 0.7495 - val_loss: 1.1835 - val_acc: 0.3790\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 49s 99ms/step - loss: 0.5045 - acc: 0.7613 - val_loss: 0.7614 - val_acc: 0.5710\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 49s 98ms/step - loss: 0.5013 - acc: 0.7615 - val_loss: 1.0172 - val_acc: 0.4110\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 49s 99ms/step - loss: 0.4945 - acc: 0.7678 - val_loss: 0.7893 - val_acc: 0.5590\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 50s 100ms/step - loss: 0.4915 - acc: 0.7678 - val_loss: 1.3331 - val_acc: 0.3480\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 51s 102ms/step - loss: 0.4870 - acc: 0.7765 - val_loss: 1.0124 - val_acc: 0.4820\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 51s 101ms/step - loss: 0.4862 - acc: 0.7730 - val_loss: 1.1912 - val_acc: 0.3930\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 50s 100ms/step - loss: 0.4791 - acc: 0.7730 - val_loss: 0.8392 - val_acc: 0.5520\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 49s 98ms/step - loss: 0.4797 - acc: 0.7765 - val_loss: 1.3104 - val_acc: 0.3330\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 50s 100ms/step - loss: 0.4755 - acc: 0.7738 - val_loss: 0.9546 - val_acc: 0.5040\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 50s 101ms/step - loss: 0.4733 - acc: 0.7835 - val_loss: 0.9221 - val_acc: 0.5100\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 50s 100ms/step - loss: 0.4637 - acc: 0.7860 - val_loss: 0.9643 - val_acc: 0.4840\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 51s 102ms/step - loss: 0.4636 - acc: 0.7853 - val_loss: 1.0904 - val_acc: 0.4470\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 50s 99ms/step - loss: 0.4617 - acc: 0.7887 - val_loss: 1.0115 - val_acc: 0.4710\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 473s 946ms/step - loss: 0.4577 - acc: 0.7908 - val_loss: 1.1386 - val_acc: 0.4420\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 52s 105ms/step - loss: 0.4592 - acc: 0.7925 - val_loss: 0.9119 - val_acc: 0.5010\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 48s 97ms/step - loss: 0.4504 - acc: 0.7970 - val_loss: 0.9641 - val_acc: 0.4850\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 48s 95ms/step - loss: 0.4549 - acc: 0.7962 - val_loss: 1.1706 - val_acc: 0.4130\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 48s 96ms/step - loss: 0.4433 - acc: 0.8010 - val_loss: 1.0855 - val_acc: 0.4600\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 50s 99ms/step - loss: 0.4408 - acc: 0.8025 - val_loss: 1.1049 - val_acc: 0.4480\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 49s 99ms/step - loss: 0.4385 - acc: 0.8027 - val_loss: 0.9558 - val_acc: 0.4940\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 50s 100ms/step - loss: 0.4367 - acc: 0.8090 - val_loss: 0.8480 - val_acc: 0.5450\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 51s 102ms/step - loss: 0.4334 - acc: 0.8067 - val_loss: 1.0469 - val_acc: 0.4910\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 55s 110ms/step - loss: 0.4313 - acc: 0.8048 - val_loss: 1.2477 - val_acc: 0.4360\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 58s 116ms/step - loss: 0.4286 - acc: 0.8095 - val_loss: 1.1542 - val_acc: 0.4440\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 55s 110ms/step - loss: 0.4203 - acc: 0.8152 - val_loss: 0.9439 - val_acc: 0.5220\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 58s 115ms/step - loss: 0.4197 - acc: 0.8175 - val_loss: 1.0831 - val_acc: 0.4650\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 55s 111ms/step - loss: 0.4155 - acc: 0.8188 - val_loss: 1.0256 - val_acc: 0.4810\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 54s 108ms/step - loss: 0.4146 - acc: 0.8225 - val_loss: 1.0082 - val_acc: 0.4840\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 51s 102ms/step - loss: 0.4194 - acc: 0.8210 - val_loss: 1.2569 - val_acc: 0.3900\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 54s 107ms/step - loss: 0.4118 - acc: 0.8260 - val_loss: 0.9434 - val_acc: 0.5160\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 55s 110ms/step - loss: 0.4021 - acc: 0.8257 - val_loss: 1.2698 - val_acc: 0.4310\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 52s 103ms/step - loss: 0.4063 - acc: 0.8250 - val_loss: 1.3738 - val_acc: 0.4150\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 49s 98ms/step - loss: 0.4057 - acc: 0.8248 - val_loss: 0.9391 - val_acc: 0.5140\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 50s 100ms/step - loss: 0.4048 - acc: 0.8285 - val_loss: 0.9835 - val_acc: 0.5140\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 47s 93ms/step - loss: 0.3960 - acc: 0.8350 - val_loss: 1.2158 - val_acc: 0.4250\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 47s 93ms/step - loss: 0.3908 - acc: 0.8382 - val_loss: 0.9936 - val_acc: 0.5270\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 53s 105ms/step - loss: 0.3955 - acc: 0.8350 - val_loss: 1.1430 - val_acc: 0.4620\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 49s 97ms/step - loss: 0.3919 - acc: 0.8355 - val_loss: 0.9569 - val_acc: 0.5540\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 49s 99ms/step - loss: 0.3882 - acc: 0.8372 - val_loss: 0.8329 - val_acc: 0.6040\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 48s 97ms/step - loss: 0.3846 - acc: 0.8365 - val_loss: 0.9878 - val_acc: 0.5040\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 53s 106ms/step - loss: 0.3837 - acc: 0.8455 - val_loss: 1.1368 - val_acc: 0.4810\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 51s 102ms/step - loss: 0.3800 - acc: 0.8415 - val_loss: 1.0237 - val_acc: 0.5080\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 48s 97ms/step - loss: 0.3745 - acc: 0.8435 - val_loss: 1.6173 - val_acc: 0.3400\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 50s 100ms/step - loss: 0.3753 - acc: 0.8460 - val_loss: 1.0720 - val_acc: 0.5060\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 50s 100ms/step - loss: 0.3731 - acc: 0.8468 - val_loss: 0.9397 - val_acc: 0.5560\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 47s 95ms/step - loss: 0.3761 - acc: 0.8430 - val_loss: 1.0276 - val_acc: 0.4940\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 49s 98ms/step - loss: 0.3712 - acc: 0.8512 - val_loss: 1.2717 - val_acc: 0.4400\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 47s 94ms/step - loss: 0.3727 - acc: 0.8443 - val_loss: 1.0489 - val_acc: 0.4910\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 50s 100ms/step - loss: 0.3668 - acc: 0.8472 - val_loss: 1.1988 - val_acc: 0.4630\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 51s 103ms/step - loss: 0.3665 - acc: 0.8540 - val_loss: 0.8033 - val_acc: 0.6080\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 45s 89ms/step - loss: 0.3632 - acc: 0.8547 - val_loss: 1.2871 - val_acc: 0.4460\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 47s 95ms/step - loss: 0.3590 - acc: 0.8562 - val_loss: 1.0455 - val_acc: 0.5190\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 49s 99ms/step - loss: 0.3566 - acc: 0.8540 - val_loss: 0.9686 - val_acc: 0.5490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "500/500 [==============================] - 57s 113ms/step - loss: 0.3510 - acc: 0.8580 - val_loss: 1.0650 - val_acc: 0.5200\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 49s 97ms/step - loss: 0.3505 - acc: 0.8635 - val_loss: 0.9675 - val_acc: 0.5570\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 48s 96ms/step - loss: 0.3511 - acc: 0.8597 - val_loss: 1.1498 - val_acc: 0.4930\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 48s 96ms/step - loss: 0.3430 - acc: 0.8620 - val_loss: 1.3971 - val_acc: 0.4020\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 51s 102ms/step - loss: 0.3442 - acc: 0.8668 - val_loss: 1.2241 - val_acc: 0.4420\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 48s 96ms/step - loss: 0.3389 - acc: 0.8622 - val_loss: 1.1675 - val_acc: 0.4860\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 47s 95ms/step - loss: 0.3413 - acc: 0.8685 - val_loss: 1.1922 - val_acc: 0.4660\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 48s 96ms/step - loss: 0.3334 - acc: 0.8752 - val_loss: 1.0612 - val_acc: 0.5520\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 51s 102ms/step - loss: 0.3341 - acc: 0.8685 - val_loss: 1.0945 - val_acc: 0.5050\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 48s 95ms/step - loss: 0.3296 - acc: 0.8695 - val_loss: 1.1890 - val_acc: 0.4820\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 46s 91ms/step - loss: 0.3207 - acc: 0.8775 - val_loss: 1.1377 - val_acc: 0.5090\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 50s 100ms/step - loss: 0.3226 - acc: 0.8765 - val_loss: 1.2637 - val_acc: 0.4760\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 47s 94ms/step - loss: 0.3180 - acc: 0.8790 - val_loss: 1.3471 - val_acc: 0.4550\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 47s 94ms/step - loss: 0.3228 - acc: 0.8808 - val_loss: 1.1463 - val_acc: 0.5230\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 47s 95ms/step - loss: 0.3239 - acc: 0.8780 - val_loss: 1.5058 - val_acc: 0.3970\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 47s 93ms/step - loss: 0.3209 - acc: 0.8765 - val_loss: 1.2086 - val_acc: 0.4950\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 50s 99ms/step - loss: 0.3162 - acc: 0.8815 - val_loss: 1.2845 - val_acc: 0.4560\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 52s 105ms/step - loss: 0.3118 - acc: 0.8823 - val_loss: 1.0587 - val_acc: 0.4900\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 44s 89ms/step - loss: 0.3105 - acc: 0.8825 - val_loss: 0.9701 - val_acc: 0.5710\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 45s 90ms/step - loss: 0.3049 - acc: 0.8855 - val_loss: 1.1764 - val_acc: 0.5070\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 45s 89ms/step - loss: 0.3015 - acc: 0.8885 - val_loss: 1.1690 - val_acc: 0.4960\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 45s 89ms/step - loss: 0.3010 - acc: 0.8875 - val_loss: 1.2972 - val_acc: 0.4820\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 45s 91ms/step - loss: 0.2991 - acc: 0.8863 - val_loss: 1.2029 - val_acc: 0.5070\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 45s 90ms/step - loss: 0.2934 - acc: 0.8930 - val_loss: 1.2970 - val_acc: 0.4840\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 44s 87ms/step - loss: 0.2927 - acc: 0.8935 - val_loss: 1.0210 - val_acc: 0.5580\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 49s 97ms/step - loss: 0.2908 - acc: 0.8920 - val_loss: 1.3316 - val_acc: 0.4630\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 47s 93ms/step - loss: 0.2878 - acc: 0.8978 - val_loss: 1.4328 - val_acc: 0.4040\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 43s 85ms/step - loss: 0.2862 - acc: 0.8970 - val_loss: 1.0442 - val_acc: 0.5380\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 44s 88ms/step - loss: 0.2783 - acc: 0.9045 - val_loss: 1.2349 - val_acc: 0.4650\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 44s 89ms/step - loss: 0.2804 - acc: 0.9032 - val_loss: 1.1868 - val_acc: 0.5040\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 43s 86ms/step - loss: 0.2765 - acc: 0.8975 - val_loss: 1.1628 - val_acc: 0.4990\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 44s 88ms/step - loss: 0.2730 - acc: 0.9035 - val_loss: 1.1270 - val_acc: 0.4850\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 44s 88ms/step - loss: 0.2689 - acc: 0.9060 - val_loss: 1.5454 - val_acc: 0.4560\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 45s 91ms/step - loss: 0.2697 - acc: 0.9072 - val_loss: 1.2163 - val_acc: 0.4970\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 46s 92ms/step - loss: 0.2713 - acc: 0.9050 - val_loss: 1.3136 - val_acc: 0.4750\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 46s 92ms/step - loss: 0.2684 - acc: 0.9035 - val_loss: 1.4438 - val_acc: 0.4370\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 46s 92ms/step - loss: 0.2624 - acc: 0.9090 - val_loss: 1.5268 - val_acc: 0.4630\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 45s 89ms/step - loss: 0.2575 - acc: 0.9118 - val_loss: 1.1463 - val_acc: 0.5090\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 45s 89ms/step - loss: 0.2615 - acc: 0.9057 - val_loss: 1.1923 - val_acc: 0.5230\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 47s 95ms/step - loss: 0.2561 - acc: 0.9097 - val_loss: 1.0423 - val_acc: 0.5520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa8314adf10>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCheckpoint = ModelCheckpoint('best_sarcasm_model.hdf5', save_best_only = True)\n",
    "model.fit(X_train_resp, y_train, batch_size=8, epochs=100, validation_split=0.2 , shuffle=True, callbacks=[modelCheckpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
